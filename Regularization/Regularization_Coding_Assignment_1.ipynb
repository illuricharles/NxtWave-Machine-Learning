{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regularization_Coding_Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "c5483236-2f07-4dc8-b75a-d5f536fa4ad9",
        "ce58da9a-b295-495c-bd8d-a3312172ca0e",
        "cd0d9306-7c58-4f4f-9ee4-e438469d58d3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "a48e6220-d6b0-4622-a00d-38b481931f66"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a06f52cf-35e9-49d7-b7ba-7715c3f5bb46"
      },
      "source": [
        "# You can add additional common utils(functions that you want to use across multiple questions) in this cell\n",
        "# This cell will be run for every question"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5483236-2f07-4dc8-b75a-d5f536fa4ad9"
      },
      "source": [
        "## Cost Function in Ridge Regression\n",
        "Implement the `compute_cost_ridge_regression()` function, which computes <br>the  cost value in Ridge regression ($L_2$ regularized linear regression) using vectorization.\n",
        "\n",
        "**Arguments:**\n",
        "\n",
        "* **`X`** : Design Matrix.\n",
        "  * A 2D numpy array of shape (num of instances, num of features)\n",
        "\n",
        "* **`Y`** : Target values corresponding to each training instance in $X$.\n",
        "  * A 2D numpy array of shape (num of instances, 1)\n",
        "\n",
        "* **`theta`** : Parameters ($\\theta_0, \\theta_1,..,\\theta_n$)\n",
        "  * A 2D numpy array of shape (num of features + 1, 1)\n",
        "\n",
        "* **`Lambda`** :  lambda($\\lambda$)\n",
        "  * A float value\n",
        "\n",
        "**Returns:**\n",
        "\n",
        "* $L_2$ regularized cost value with the given parameters and data\n",
        "<br><br>$\\hspace{20mm}J(\\theta) = \\frac{1}{2m}(X\\theta - Y)^T(X\\theta-Y) + \\frac{\\lambda}{2m}\\theta^TI'\\theta\\\\[0.5pt]$\n",
        "<br>$\\hspace{2cm}$(where $I'$ is an identity matrix with the first element as 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a098e73b-f3da-416c-ae27-71fa449204ae"
      },
      "source": [
        "def compute_cost_ridge_regression(X, Y, theta, Lambda):\n",
        "  #ADD YOUR CODE HERE\n",
        "  X = np.insert(X,0,1,axis = 1)\n",
        "  H = np.dot(X,theta)\n",
        "  diff = H-Y\n",
        "  m = len(X)\n",
        "  I = np.identity(len(theta))\n",
        "  I[0][0] = 0\n",
        "  cost_term = (1/(2*m)) * np.dot(diff.T,diff)\n",
        "  reg_term = (Lambda/(2*m)) * np.dot(np.dot(theta.T,I), theta)\n",
        "  J = cost_term + reg_term\n",
        "  return np.squeeze(J)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db1a454f-9e54-4b2a-a496-fe9d859e593f",
        "outputId": "2f92eed1-b8b6-4e06-fc47-a68ce22b13f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#SAMPLE TEST CASE\n",
        "X = np.array([[5, 2, 7.3, 6],\n",
        "             [6, 1, 0.2, 4],\n",
        "             [10, 12.1, 1.4, 4]])\n",
        "Y = np.array([[0.2], [1.1], [2.3]])\n",
        "theta = np.array([[0.1], [0.3], [-0.5], [-0.2], [0.4]])\n",
        "Lambda = 0.1\n",
        "print(np.round(compute_cost_ridge_regression(X, Y, theta, Lambda),3))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4438e9b5-de8c-4f42-9360-d1c150a8c4a9"
      },
      "source": [
        "**Expected Output:**\n",
        "```\n",
        "3.459\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce58da9a-b295-495c-bd8d-a3312172ca0e"
      },
      "source": [
        "## Cost Function in Lasso Regression\n",
        "Implement the `compute_cost_lasso_regression()` function, which computes<br> the cost value in Lasso regression ($L_1$ regularized linear regression).\n",
        "\n",
        "**Arguments:**\n",
        "\n",
        "* **`X`** : Design Matrix.\n",
        "  * A 2D numpy array of shape (num of instances, num of features)\n",
        "\n",
        "* **`Y`** : Target values corresponding to each training instance in $X$.\n",
        "  * A 2D numpy array of shape (num of instances, 1)\n",
        "\n",
        "* **`theta`** : Parameters ($\\theta_0, \\theta_1,..,\\theta_n$)\n",
        "  * A 2D numpy array of shape (num of features + 1, 1)\n",
        "\n",
        "* **`Lambda`** :  Regularization parameter($\\lambda$)\n",
        "  * A float value\n",
        "\n",
        "**Returns:**\n",
        "\n",
        "* $L_1$ regularized cost value with the given parameters and data\n",
        "<br><br>$\\hspace{20mm}J(\\theta) = \\frac{1}{2m}(X\\theta - Y)^T(X\\theta-Y) + \\frac{\\lambda}{2m}\\sum_{j=1}^n|\\theta_j|$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "892e132b-8864-402e-b98f-6f579b029c03"
      },
      "source": [
        "from os import XATTR_CREATE\n",
        "def compute_cost_lasso_regression(X, Y, theta, Lambda):\n",
        "  #ADD YOUR CODE HERE\n",
        "  X = np.insert(X,0,1,axis = 1)\n",
        "  H = np.dot(X,theta)\n",
        "  diff = H-Y\n",
        "  m = len(X)\n",
        "  cost_term = (1/(2*m)) * np.dot(diff.T, diff) \n",
        "  theta_abs = np.absolute(theta[1:])\n",
        "  reg_term = (Lambda/(2*m)) * np.sum(theta_abs)\n",
        "  J = cost_term + reg_term\n",
        "  return np.squeeze(J)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cfaaa5d-069d-4d90-9b34-1dd18bc8f05a",
        "outputId": "4a31ea96-d776-4061-e298-7068930aa0d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#SAMPLE TEST CASE\n",
        "X = np.array([[5, 2, 7.3, 6],\n",
        "             [6, 1, 0.2, 4],\n",
        "             [10, 12.1, 1.4, 4]])\n",
        "Y = np.array([[0.2], [1.1], [2.3]])\n",
        "theta = np.array([[0.1], [0.3], [-0.5], [-0.2], [0.4]])\n",
        "Lambda = 0.1\n",
        "print(np.round(compute_cost_lasso_regression(X, Y, theta, Lambda),3))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba6a6a19-a407-437f-85c6-fc805507d99d"
      },
      "source": [
        "**Expected Output:**\n",
        "```\n",
        "3.473\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd0d9306-7c58-4f4f-9ee4-e438469d58d3"
      },
      "source": [
        "## Gradient Descent in Ridge Regression\n",
        "Implement the `gradient_descent_ridge_regression()` function, which computes the optimal parameter values using gradient descent in Ridge regression.\n",
        "\n",
        "**Arguments:**\n",
        "\n",
        "* **`X`** : Design Matrix.\n",
        "  * A 2D numpy array of shape (num of instances, num of features)\n",
        "\n",
        "* **`Y`** : Target values corresponding to each training instance in $X$.\n",
        "  * A 2D numpy array of shape (num of instances, 1)\n",
        "\n",
        "* **`theta`** : Parameters ($\\theta_0, \\theta_1,..,\\theta_n$)\n",
        "  * A 2D numpy array of shape (num of features + 1, 1)\n",
        "\n",
        "* **`Lambda`** :  Regularization parameter($\\lambda$)\n",
        "  * A float value\n",
        "\n",
        "* **`cost_diff_threshold`** : threshold of the absolute cost difference to stop iterating in gradient descent\n",
        "  * A float value\n",
        "\n",
        "* **`learning_rate`** :  Learning rate($\\alpha$)\n",
        "  * A float value\n",
        "\n",
        "**Returns:**\n",
        "\n",
        "* Optimal parameters with the given data <br><br>$\\hspace{20mm}\\theta = \\theta -  \\frac{\\alpha}{m}[X^T(X\\theta-Y) + \\lambda I'\\theta]\\\\[0.1pt]$\n",
        "<br>$\\hspace{2cm}$(where $I'$ is an identity matrix with the first element as 0)\n",
        "\n",
        "\n",
        "**NOTE:**\n",
        "* The gradient descent is said to be converged when the absolute value of the cost difference is less than the given threshold.\n",
        "* Stop iterating when the gradient descent starts to diverge.\n",
        "* You can call `compute_cost_ridge_regression()` implemented in the previous question to compute the cost value for convergence check."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1efa0a62-6797-42c9-bc3a-505028137e86"
      },
      "source": [
        "def gradient_descent_cost_term(X, Y, theta, Lambda):\n",
        "  X = np.insert(X,0,1,axis = 1)\n",
        "  H = np.dot(X, theta)\n",
        "  diff = H - Y\n",
        "  cost_term = np.dot(X.T, diff)\n",
        "  I = np.identity(len(theta))\n",
        "  I[0][0] = 0\n",
        "  reg_term = Lambda * (np.dot(I, theta))\n",
        "  m = len(X)\n",
        "  d_theta = (1/m) * (np.dot(X.T, diff) + reg_term)\n",
        "  return d_theta\n",
        "\n",
        "def gradient_descent_ridge_regression(X, Y, theta, Lambda, cost_diff_threshold, learning_rate):\n",
        "   i=0\n",
        "   cost_diff = cost_diff_threshold + 1\n",
        "   costs = [compute_cost_ridge_regression(X, Y, theta, Lambda)]\n",
        "\n",
        "   while(abs(cost_diff) > cost_diff_threshold):\n",
        "     d_theta = gradient_descent_cost_term(X, Y, theta, Lambda)\n",
        "     theta = theta - (learning_rate * d_theta)\n",
        "     costs.append(compute_cost_ridge_regression(X, Y, theta, Lambda))\n",
        "     cost_diff = costs[i+1] - costs[i]\n",
        "     if cost_diff > 0:\n",
        "       break\n",
        "     i = i+1\n",
        "   \n",
        "   return theta"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f527f9a-7106-4f50-9f0d-eb4b24e7a8c7",
        "outputId": "ae42c4e8-fcbf-4edd-f67f-9864900e73c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#SAMPLE TEST CASE\n",
        "X = np.array([[5, 2, 7.3, 6],\n",
        "             [6, 1, 0.2, 4],\n",
        "             [10, 12.1, 1.4, 4]])\n",
        "Y = np.array([[0.2], [1.1], [2.3]])\n",
        "theta = np.array([[0.8], [0.1], [0.1], [-0.1], [-0.1]])\n",
        "\n",
        "Lambda = 0.9\n",
        "cost_diff_threshold =1e-11\n",
        "learning_rate = 0.01\n",
        "\n",
        "optimal_theta = np.round(gradient_descent_ridge_regression(X, Y, theta, Lambda, cost_diff_threshold, learning_rate),3)\n",
        "print(*(optimal_theta.flatten()))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.801 0.061 0.099 -0.118 -0.037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b803ca3-660f-49cd-9296-33da5d28719d"
      },
      "source": [
        "**Expected Output:**\n",
        "```\n",
        "0.801 0.061 0.099 -0.118 -0.037\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24980ee9-1ae1-48b6-ab89-cd6469c5e489"
      },
      "source": [
        "## Normal Equation method in Ridge Regression\n",
        "Implement the `normal_equation_ridge_regression()` function, which computes the optimal parameter values using the Normal Equation method in Ridge regression.\n",
        "\n",
        "**Arguments:**\n",
        "\n",
        "* **`X`** : Design Matrix.\n",
        "  * A 2D numpy array of shape (num of instances, num of features)\n",
        "\n",
        "* **`Y`** : Target values corresponding to each training instance in $X$.\n",
        "  * A 2D numpy array of shape (num of instances, 1)\n",
        "\n",
        "\n",
        "* **`Lambda`** : Regularization parameter($\\lambda$)\n",
        "  * A float value\n",
        "\n",
        "\n",
        "**Returns:**\n",
        "\n",
        "* Optimal parameters with the given data\n",
        "<br><br>$\\hspace{20mm}\\theta = (X^TX+\\lambda I')^{-1} X^TY\\\\[0.5pt]$\n",
        "<br>$\\hspace{2cm}$(where $I'$ is an identity matrix with the first element as 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4d17a55-f8aa-457c-9d67-cb7b7a8cf609"
      },
      "source": [
        "def normal_equation_ridge_regression(X, Y, Lambda):\n",
        "  #ADD YOUR CODE HERE\n",
        "  X = np.insert(X,0,1,axis = 1)\n",
        "  xtx = np.dot(X.T,X)\n",
        "  n = X.shape\n",
        "  I = np.identity((n[1]))\n",
        "  I[0][0] = 0\n",
        "  li = Lambda * I\n",
        "  inverse = np.linalg.inv(xtx + li)\n",
        "  xty = np.dot(X.T,Y)\n",
        "  theta = np.dot(inverse, xty)\n",
        "  return theta\n",
        "  "
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a7bfc41-2ebc-4740-9e03-9f71cedcb53c",
        "outputId": "7f6825e9-67af-4f5c-ea3e-b85688c0dd3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#SAMPLE TEST CASE\n",
        "X = np.array([[5, 2, 7.3, 6],\n",
        "             [6, 1, 0.2, 4],\n",
        "             [10, 12.1, 1.4, 4]])\n",
        "Y = np.array([[0.2], [1.1], [2.3]])\n",
        "Lambda = 0.9\n",
        "\n",
        "print(*np.round(normal_equation_ridge_regression(X, Y, Lambda).flatten(),3))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.801 0.061 0.099 -0.118 -0.037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36b771df-143b-4597-96e9-dce46e381c5f"
      },
      "source": [
        "**Expected Output:**\n",
        "```\n",
        "0.801 0.061 0.099 -0.118 -0.037\n",
        "```"
      ]
    }
  ]
}